{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WXu1r8qvSzWf",
    "tags": []
   },
   "source": [
    "# Twin-Delayed DDPG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YRzQUhuUTc0J"
   },
   "source": [
    "## Installing the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HAHMB0Ze8fU0",
    "outputId": "d3063bcb-9e38-4438-b2a9-463f293b3a13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pybullet in c:\\users\\mudit\\anaconda3\\lib\\site-packages (3.2.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install pybullet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xjm2onHdT-Av"
   },
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Ikr2p0Js8iB4"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pybullet_envs\n",
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from gym import wrappers\n",
    "from torch.autograd import Variable\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y2nGdtlKVydr"
   },
   "source": [
    "## Step 1: We initialize the Experience Replay memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "u5rW0IDB8nTO"
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "\n",
    "  def __init__(self, max_size=1e6):\n",
    "    self.storage = []\n",
    "    self.max_size = max_size\n",
    "    self.ptr = 0\n",
    "\n",
    "  def add(self, transition):\n",
    "    if len(self.storage) == self.max_size:\n",
    "      self.storage[int(self.ptr)] = transition\n",
    "      self.ptr = (self.ptr + 1) % self.max_size\n",
    "    else:\n",
    "      self.storage.append(transition)\n",
    "\n",
    "  def sample(self, batch_size):\n",
    "    ind = np.random.randint(0, len(self.storage), size=batch_size)\n",
    "    batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = [], [], [], [], []\n",
    "    for i in ind: \n",
    "      state, next_state, action, reward, done = self.storage[i]\n",
    "      batch_states.append(np.array(state, copy=False))\n",
    "      batch_next_states.append(np.array(next_state, copy=False))\n",
    "      batch_actions.append(np.array(action, copy=False))\n",
    "      batch_rewards.append(np.array(reward, copy=False))\n",
    "      batch_dones.append(np.array(done, copy=False))\n",
    "    return np.array(batch_states), np.array(batch_next_states), np.array(batch_actions), np.array(batch_rewards).reshape(-1, 1), np.array(batch_dones).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jb7TTaHxWbQD"
   },
   "source": [
    "## Step 2: We build one neural network for the Actor model and one neural network for the Actor target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "4CeRW4D79HL0"
   },
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "  \n",
    "  def __init__(self, state_dim, action_dim, max_action):\n",
    "    super(Actor, self).__init__()\n",
    "    self.layer_1 = nn.Linear(state_dim, 400)\n",
    "    self.layer_2 = nn.Linear(400, 300)\n",
    "    self.layer_3 = nn.Linear(300, action_dim)\n",
    "    self.max_action = max_action\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = F.relu(self.layer_1(x))\n",
    "    x = F.relu(self.layer_2(x))\n",
    "    x = self.max_action * torch.tanh(self.layer_3(x))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HRDDce8FXef7"
   },
   "source": [
    "## Step 3: We build two neural networks for the two Critic models and two neural networks for the two Critic targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "OCee7gwR9Jrs"
   },
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "  \n",
    "  def __init__(self, state_dim, action_dim):\n",
    "    super(Critic, self).__init__()\n",
    "    # Defining the first Critic neural network\n",
    "    self.layer_1 = nn.Linear(state_dim + action_dim, 400)\n",
    "    self.layer_2 = nn.Linear(400, 300)\n",
    "    self.layer_3 = nn.Linear(300, 1)\n",
    "    # Defining the second Critic neural network\n",
    "    self.layer_4 = nn.Linear(state_dim + action_dim, 400)\n",
    "    self.layer_5 = nn.Linear(400, 300)\n",
    "    self.layer_6 = nn.Linear(300, 1)\n",
    "\n",
    "  def forward(self, x, u):\n",
    "    xu = torch.cat([x, u], 1)\n",
    "    # Forward-Propagation on the first Critic Neural Network\n",
    "    x1 = F.relu(self.layer_1(xu))\n",
    "    x1 = F.relu(self.layer_2(x1))\n",
    "    x1 = self.layer_3(x1)\n",
    "    # Forward-Propagation on the second Critic Neural Network\n",
    "    x2 = F.relu(self.layer_4(xu))\n",
    "    x2 = F.relu(self.layer_5(x2))\n",
    "    x2 = self.layer_6(x2)\n",
    "    return x1, x2\n",
    "\n",
    "  def Q1(self, x, u):\n",
    "    xu = torch.cat([x, u], 1)\n",
    "    x1 = F.relu(self.layer_1(xu))\n",
    "    x1 = F.relu(self.layer_2(x1))\n",
    "    x1 = self.layer_3(x1)\n",
    "    return x1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NzIDuONodenW"
   },
   "source": [
    "## Steps 4 to 15: Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "zzd0H1xukdKe"
   },
   "outputs": [],
   "source": [
    "# Selecting the device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Building the whole Training Process into a class\n",
    "\n",
    "class TD3(object):\n",
    "  \n",
    "  def __init__(self, state_dim, action_dim, max_action):\n",
    "    self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
    "    self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
    "    self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "    self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n",
    "    self.critic = Critic(state_dim, action_dim).to(device)\n",
    "    self.critic_target = Critic(state_dim, action_dim).to(device)\n",
    "    self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "    self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
    "    self.max_action = max_action\n",
    "\n",
    "  def select_action(self, state):\n",
    "    state = torch.Tensor(state.reshape(1, -1)).to(device)\n",
    "    return self.actor(state).cpu().data.numpy().flatten()\n",
    "\n",
    "  def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
    "    \n",
    "    for it in range(iterations):\n",
    "      \n",
    "      # Step 4: We sample a batch of transitions (s, s’, a, r) from the memory\n",
    "      batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
    "      state = torch.Tensor(batch_states).to(device)\n",
    "      next_state = torch.Tensor(batch_next_states).to(device)\n",
    "      action = torch.Tensor(batch_actions).to(device)\n",
    "      reward = torch.Tensor(batch_rewards).to(device)\n",
    "      done = torch.Tensor(batch_dones).to(device)\n",
    "      \n",
    "      # Step 5: From the next state s’, the Actor target plays the next action a’\n",
    "      next_action = self.actor_target(next_state)\n",
    "      \n",
    "      # Step 6: We add Gaussian noise to this next action a’ and we clamp it in a range of values supported by the environment\n",
    "      noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)\n",
    "      noise = noise.clamp(-noise_clip, noise_clip)\n",
    "      next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n",
    "      \n",
    "      # Step 7: The two Critic targets take each the couple (s’, a’) as input and return two Q-values Qt1(s’,a’) and Qt2(s’,a’) as outputs\n",
    "      target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
    "      \n",
    "      # Step 8: We keep the minimum of these two Q-values: min(Qt1, Qt2)\n",
    "      target_Q = target_Q1\n",
    "      \n",
    "      # Step 9: We get the final target of the two Critic models, which is: Qt = r + γ * min(Qt1, Qt2), where γ is the discount factor\n",
    "      target_Q = reward + ((1 - done) * discount * target_Q).detach()\n",
    "      \n",
    "      # Step 10: The two Critic models take each the couple (s, a) as input and return two Q-values Q1(s,a) and Q2(s,a) as outputs\n",
    "      current_Q1, current_Q2 = self.critic(state, action)\n",
    "      \n",
    "      # Step 11: We compute the loss coming from the two Critic models: Critic Loss = MSE_Loss(Q1(s,a), Qt) + MSE_Loss(Q2(s,a), Qt)\n",
    "      critic_loss = F.mse_loss(current_Q1, target_Q) \n",
    "      \n",
    "      # Step 12: We backpropagate this Critic loss and update the parameters of the two Critic models with a SGD optimizer\n",
    "      self.critic_optimizer.zero_grad()\n",
    "      critic_loss.backward()\n",
    "      self.critic_optimizer.step()\n",
    "      \n",
    "      # Step 13: Once every two iterations, we update our Actor model by performing gradient ascent on the output of the first Critic model\n",
    "      if it % policy_freq == 0:\n",
    "        actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        # Step 14: Still once every two iterations, we update the weights of the Actor target by polyak averaging\n",
    "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "        \n",
    "        # Step 15: Still once every two iterations, we update the weights of the Critic target by polyak averaging\n",
    "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "          target_param.data.copy_(tau_1 * param.data + (1 - tau_1) * target_param.data)\n",
    "  \n",
    "  # Making a save method to save a trained model\n",
    "  def save(self, filename, directory):\n",
    "    torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
    "    torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
    "  \n",
    "  # Making a load method to load a pre-trained model\n",
    "  def load(self, filename, directory):\n",
    "    self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
    "    self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ka-ZRtQvjBex"
   },
   "source": [
    "## We make a function that evaluates the policy by calculating its average reward over 10 episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "qabqiYdp9wDM"
   },
   "outputs": [],
   "source": [
    "def evaluate_policy(policy, eval_episodes=10):\n",
    "  avg_reward = 0.\n",
    "  for _ in range(eval_episodes):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "      action = policy.select_action(np.array(obs))\n",
    "      obs, reward, done, _ = env.step(action)\n",
    "      avg_reward += reward\n",
    "  avg_reward /= eval_episodes\n",
    "  print (\"---------------------------------------\")\n",
    "  print (\"Average Reward over the Evaluation Step: %f\" % (avg_reward))\n",
    "  print (\"---------------------------------------\")\n",
    "  return avg_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gGuKmH_ijf7U"
   },
   "source": [
    "## We set the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "HFj6wbAo97lk"
   },
   "outputs": [],
   "source": [
    "env_name = \"AntBulletEnv-v0\" # Name of a environment (set it to any Continous environment you want)\n",
    "seed = 0   # Random seed number\n",
    "start_timesteps = 1e4 # Number of iterations/timesteps before which the model randomly chooses an action, and after which it starts to use the policy network\n",
    "eval_freq = 2e3 # How often the evaluation step is performed (after how many timesteps)\n",
    "max_timesteps = 6e5 # Total number of iterations/timesteps\n",
    "save_models = True # Boolean checker whether or not to save the pre-trained model\n",
    "expl_noise = 0.1 # Exploration noise - STD value of exploration Gaussian noise\n",
    "batch_size = 100 # Size of the batch\n",
    "discount = 0.99 # Discount factor gamma, used in the calculation of the total discounted reward\n",
    "tau_1 = 0.005 # Target network update rate\n",
    "tau = 0.005\n",
    "policy_noise = 0.2 # STD of Gaussian noise added to the actions for the exploration purposes\n",
    "noise_clip = 0.5 # Maximum value of the Gaussian noise added to the actions (policy)\n",
    "policy_freq = 3 # Number of iterations to wait before the policy network (Actor model) is updated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hjwf2HCol3XP"
   },
   "source": [
    "## We create a file name for the two saved models: the Actor and Critic models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1fyH8N5z-o3o",
    "outputId": "57ef6a51-7fd6-48a7-a29d-ea94f122d302"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Settings: TD3_AntBulletEnv-v0_0\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "file_name = \"%s_%s_%s\" % (\"TD3\", env_name, str(seed))\n",
    "print (\"---------------------------------------\")\n",
    "print (\"Settings: %s\" % (file_name))\n",
    "print (\"---------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kop-C96Aml8O"
   },
   "source": [
    "## We create a folder inside which will be saved the trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "Src07lvY-zXb"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(\"./results\"):\n",
    "  os.makedirs(\"./results\")\n",
    "if save_models and not os.path.exists(\"./pytorch_models\"):\n",
    "  os.makedirs(\"./pytorch_models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qEAzOd47mv1Z"
   },
   "source": [
    "## We create the PyBullet environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CyQXJUIs-6BV",
    "outputId": "7920ca2b-8e7b-425c-d5d9-e3fc6568a2be"
   },
   "outputs": [],
   "source": [
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5YdPG4HXnNsh"
   },
   "source": [
    "## We set seeds and we get the necessary information on the states and actions in the chosen environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "Z3RufYec_ADj"
   },
   "outputs": [],
   "source": [
    "env.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "max_action = float(env.action_space.high[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HWEgDAQxnbem"
   },
   "source": [
    "## We create the policy network (the Actor model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "wTVvG7F8_EWg"
   },
   "outputs": [],
   "source": [
    "policy = TD3(state_dim, action_dim, max_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZI60VN2Unklh"
   },
   "source": [
    "## We create the Experience Replay memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "sd-ZsdXR_LgV"
   },
   "outputs": [],
   "source": [
    "replay_buffer = ReplayBuffer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QYOpCyiDnw7s"
   },
   "source": [
    "## We define a list where all the evaluation results over 10 episodes are stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dhC_5XJ__Orp",
    "outputId": "64fca46e-0ed3-4f61-f082-2b16fccf256e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 9.804960\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "evaluations = [evaluate_policy(policy)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xm-4b3p6rglE"
   },
   "source": [
    "## We create a new folder directory in which the final results (videos of the agent) will be populated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "MTL9uMd0ru03"
   },
   "outputs": [],
   "source": [
    "def mkdir(base, name):\n",
    "    path = os.path.join(base, name)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    return path\n",
    "work_dir = mkdir('exp', 'brs')\n",
    "monitor_dir = mkdir(work_dir, 'monitor')\n",
    "max_episode_steps = env._max_episode_steps\n",
    "save_env_vid = False\n",
    "if save_env_vid:\n",
    "  env = wrappers.Monitor(env, monitor_dir, force = True)\n",
    "  env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "31n5eb03p-Fm"
   },
   "source": [
    "## We initialize the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "1vN5EvxK_QhT"
   },
   "outputs": [],
   "source": [
    "total_timesteps = 0\n",
    "timesteps_since_eval = 0\n",
    "episode_num = 0\n",
    "done = True\n",
    "t0 = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q9gsjvtPqLgT"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "y_ouY4NH_Y0I"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Timesteps: 500001 Episode Num: 603 Reward: 386.6404688617539\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 618.355644\n",
      "---------------------------------------\n",
      "Total Timesteps: 501001 Episode Num: 604 Reward: 668.1590937681681\n",
      "Total Timesteps: 502001 Episode Num: 605 Reward: 765.3461017393535\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 562.273872\n",
      "---------------------------------------\n",
      "Total Timesteps: 503001 Episode Num: 606 Reward: 546.1001935822038\n",
      "Total Timesteps: 504001 Episode Num: 607 Reward: 448.28852622916656\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 613.391015\n",
      "---------------------------------------\n",
      "Total Timesteps: 505001 Episode Num: 608 Reward: 675.6564311693231\n",
      "Total Timesteps: 506001 Episode Num: 609 Reward: 410.4362464822965\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 559.706464\n",
      "---------------------------------------\n",
      "Total Timesteps: 507001 Episode Num: 610 Reward: 439.1606168713617\n",
      "Total Timesteps: 508001 Episode Num: 611 Reward: 284.17868734247475\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 500.871638\n",
      "---------------------------------------\n",
      "Total Timesteps: 509001 Episode Num: 612 Reward: 691.7151069499743\n",
      "Total Timesteps: 510001 Episode Num: 613 Reward: 687.8831476283939\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 555.795578\n",
      "---------------------------------------\n",
      "Total Timesteps: 511001 Episode Num: 614 Reward: 648.6965073844924\n",
      "Total Timesteps: 512001 Episode Num: 615 Reward: 720.5291220011355\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 672.499991\n",
      "---------------------------------------\n",
      "Total Timesteps: 513001 Episode Num: 616 Reward: 748.700919965849\n",
      "Total Timesteps: 514001 Episode Num: 617 Reward: 670.161453026701\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 694.888605\n",
      "---------------------------------------\n",
      "Total Timesteps: 515001 Episode Num: 618 Reward: 803.989002251238\n",
      "Total Timesteps: 516001 Episode Num: 619 Reward: 804.599811371187\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 733.297337\n",
      "---------------------------------------\n",
      "Total Timesteps: 517001 Episode Num: 620 Reward: 767.6462114380772\n",
      "Total Timesteps: 518001 Episode Num: 621 Reward: 805.6617520822059\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 740.740868\n",
      "---------------------------------------\n",
      "Total Timesteps: 519001 Episode Num: 622 Reward: 669.83797979663\n",
      "Total Timesteps: 520001 Episode Num: 623 Reward: 741.4945674440582\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 736.562293\n",
      "---------------------------------------\n",
      "Total Timesteps: 521001 Episode Num: 624 Reward: 716.6644151662048\n",
      "Total Timesteps: 522001 Episode Num: 625 Reward: 776.7523323217061\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 823.968466\n",
      "---------------------------------------\n",
      "Total Timesteps: 523001 Episode Num: 626 Reward: 830.5785477827951\n",
      "Total Timesteps: 524001 Episode Num: 627 Reward: 611.6096505265466\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 641.593193\n",
      "---------------------------------------\n",
      "Total Timesteps: 525001 Episode Num: 628 Reward: 863.8358122986222\n",
      "Total Timesteps: 526001 Episode Num: 629 Reward: 752.3915314455091\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 600.212356\n",
      "---------------------------------------\n",
      "Total Timesteps: 527001 Episode Num: 630 Reward: 639.3551719023183\n",
      "Total Timesteps: 528001 Episode Num: 631 Reward: 598.8812521746551\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 902.031649\n",
      "---------------------------------------\n",
      "Total Timesteps: 529001 Episode Num: 632 Reward: 810.3748846593429\n",
      "Total Timesteps: 530001 Episode Num: 633 Reward: 630.2247071727616\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 764.388831\n",
      "---------------------------------------\n",
      "Total Timesteps: 531001 Episode Num: 634 Reward: 817.5886239016085\n",
      "Total Timesteps: 532001 Episode Num: 635 Reward: 692.0829744321449\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 785.659359\n",
      "---------------------------------------\n",
      "Total Timesteps: 533001 Episode Num: 636 Reward: 843.1024181454098\n",
      "Total Timesteps: 534001 Episode Num: 637 Reward: 922.4812837478472\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 708.554335\n",
      "---------------------------------------\n",
      "Total Timesteps: 535001 Episode Num: 638 Reward: 470.38493929936766\n",
      "Total Timesteps: 536001 Episode Num: 639 Reward: 504.10559548852973\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 547.070068\n",
      "---------------------------------------\n",
      "Total Timesteps: 537001 Episode Num: 640 Reward: 359.5427097566433\n",
      "Total Timesteps: 538001 Episode Num: 641 Reward: 577.2844971485326\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 792.429500\n",
      "---------------------------------------\n",
      "Total Timesteps: 539001 Episode Num: 642 Reward: 772.7986346123065\n",
      "Total Timesteps: 540001 Episode Num: 643 Reward: 508.7334401314872\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 806.215803\n",
      "---------------------------------------\n",
      "Total Timesteps: 541001 Episode Num: 644 Reward: 1148.544282756758\n",
      "Total Timesteps: 542001 Episode Num: 645 Reward: 998.3613117709046\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 826.896778\n",
      "---------------------------------------\n",
      "Total Timesteps: 543001 Episode Num: 646 Reward: 792.4152191525226\n",
      "Total Timesteps: 544001 Episode Num: 647 Reward: 667.3572061190561\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 979.914804\n",
      "---------------------------------------\n",
      "Total Timesteps: 545001 Episode Num: 648 Reward: 707.6661936603625\n",
      "Total Timesteps: 546001 Episode Num: 649 Reward: 1103.8460588147195\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 904.317707\n",
      "---------------------------------------\n",
      "Total Timesteps: 547001 Episode Num: 650 Reward: 919.9582695768987\n",
      "Total Timesteps: 548001 Episode Num: 651 Reward: 1157.6254269499943\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 897.100337\n",
      "---------------------------------------\n",
      "Total Timesteps: 549001 Episode Num: 652 Reward: 1054.8000532375104\n",
      "Total Timesteps: 550001 Episode Num: 653 Reward: 1041.5314694310505\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1023.687076\n",
      "---------------------------------------\n",
      "Total Timesteps: 551001 Episode Num: 654 Reward: 954.5246156181109\n",
      "Total Timesteps: 552001 Episode Num: 655 Reward: 598.0142699775297\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 551.093257\n",
      "---------------------------------------\n",
      "Total Timesteps: 553001 Episode Num: 656 Reward: 516.7091084497968\n",
      "Total Timesteps: 554001 Episode Num: 657 Reward: 540.2262624441435\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 876.981314\n",
      "---------------------------------------\n",
      "Total Timesteps: 555001 Episode Num: 658 Reward: 858.695114471428\n",
      "Total Timesteps: 556001 Episode Num: 659 Reward: 1210.8605768893224\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 698.757853\n",
      "---------------------------------------\n",
      "Total Timesteps: 557001 Episode Num: 660 Reward: 806.7942249460503\n",
      "Total Timesteps: 558001 Episode Num: 661 Reward: 698.0443467915508\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 639.306258\n",
      "---------------------------------------\n",
      "Total Timesteps: 559001 Episode Num: 662 Reward: 921.3946267290808\n",
      "Total Timesteps: 560001 Episode Num: 663 Reward: 794.9010261794034\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 683.563217\n",
      "---------------------------------------\n",
      "Total Timesteps: 561001 Episode Num: 664 Reward: 874.5416157777845\n",
      "Total Timesteps: 562001 Episode Num: 665 Reward: 836.7991630162406\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 778.810028\n",
      "---------------------------------------\n",
      "Total Timesteps: 563001 Episode Num: 666 Reward: 864.3601914825075\n",
      "Total Timesteps: 564001 Episode Num: 667 Reward: 841.519919188388\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 596.306607\n",
      "---------------------------------------\n",
      "Total Timesteps: 564235 Episode Num: 668 Reward: 112.21583894597727\n",
      "Total Timesteps: 564580 Episode Num: 669 Reward: 215.95409255004586\n",
      "Total Timesteps: 565580 Episode Num: 670 Reward: 627.2702553791999\n",
      "Total Timesteps: 566580 Episode Num: 671 Reward: 794.2585519581687\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 607.025163\n",
      "---------------------------------------\n",
      "Total Timesteps: 567580 Episode Num: 672 Reward: 840.7155263971257\n",
      "Total Timesteps: 568580 Episode Num: 673 Reward: 834.7346110072831\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 561.780922\n",
      "---------------------------------------\n",
      "Total Timesteps: 568689 Episode Num: 674 Reward: 59.1155374060485\n",
      "Total Timesteps: 568804 Episode Num: 675 Reward: 57.79297625667743\n",
      "Total Timesteps: 568941 Episode Num: 676 Reward: 68.34244098548838\n",
      "Total Timesteps: 569941 Episode Num: 677 Reward: 496.3733611494304\n",
      "Total Timesteps: 570745 Episode Num: 678 Reward: 688.1134109113057\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 419.739514\n",
      "---------------------------------------\n",
      "Total Timesteps: 571564 Episode Num: 679 Reward: 442.5132486597755\n",
      "Total Timesteps: 571873 Episode Num: 680 Reward: 245.65268930693867\n",
      "Total Timesteps: 572873 Episode Num: 681 Reward: 239.04091516302952\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 185.663678\n",
      "---------------------------------------\n",
      "Total Timesteps: 573873 Episode Num: 682 Reward: 228.20316618864493\n",
      "Total Timesteps: 573976 Episode Num: 683 Reward: 63.27925536047964\n",
      "Total Timesteps: 574110 Episode Num: 684 Reward: 70.81110020452158\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 213.545173\n",
      "---------------------------------------\n",
      "Total Timesteps: 575110 Episode Num: 685 Reward: 246.77547729863815\n",
      "Total Timesteps: 575285 Episode Num: 686 Reward: 94.5560080186192\n",
      "Total Timesteps: 575418 Episode Num: 687 Reward: 78.31038512798314\n",
      "Total Timesteps: 575581 Episode Num: 688 Reward: 69.29156151671198\n",
      "Total Timesteps: 575741 Episode Num: 689 Reward: 103.0734277947199\n",
      "Total Timesteps: 576741 Episode Num: 690 Reward: 499.1112445518579\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 283.515806\n",
      "---------------------------------------\n",
      "Total Timesteps: 577741 Episode Num: 691 Reward: 391.5437912907527\n",
      "Total Timesteps: 577873 Episode Num: 692 Reward: 68.66535590961473\n",
      "Total Timesteps: 578873 Episode Num: 693 Reward: 619.8639540890564\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 113.581116\n",
      "---------------------------------------\n",
      "Total Timesteps: 578991 Episode Num: 694 Reward: 59.52894494047769\n",
      "Total Timesteps: 579123 Episode Num: 695 Reward: 72.2150852074234\n",
      "Total Timesteps: 579253 Episode Num: 696 Reward: 77.21570033685911\n",
      "Total Timesteps: 579396 Episode Num: 697 Reward: 86.21715077134127\n",
      "Total Timesteps: 579528 Episode Num: 698 Reward: 74.31513758268812\n",
      "Total Timesteps: 579642 Episode Num: 699 Reward: 66.62847878519324\n",
      "Total Timesteps: 580642 Episode Num: 700 Reward: 469.31127508791053\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 186.192240\n",
      "---------------------------------------\n",
      "Total Timesteps: 581642 Episode Num: 701 Reward: 260.2281741290868\n",
      "Total Timesteps: 582642 Episode Num: 702 Reward: 269.46286964906926\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 115.124022\n",
      "---------------------------------------\n",
      "Total Timesteps: 583642 Episode Num: 703 Reward: 266.73914603410134\n",
      "Total Timesteps: 583768 Episode Num: 704 Reward: 74.4840929373104\n",
      "Total Timesteps: 584768 Episode Num: 705 Reward: 247.495271236571\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 163.617738\n",
      "---------------------------------------\n",
      "Total Timesteps: 585768 Episode Num: 706 Reward: 308.656910775766\n",
      "Total Timesteps: 585877 Episode Num: 707 Reward: 55.548037215826724\n",
      "Total Timesteps: 586877 Episode Num: 708 Reward: 298.88083067055754\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 303.481357\n",
      "---------------------------------------\n",
      "Total Timesteps: 587877 Episode Num: 709 Reward: 386.52727660203124\n",
      "Total Timesteps: 588029 Episode Num: 710 Reward: 87.61435783337596\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 239.625115\n",
      "---------------------------------------\n",
      "Total Timesteps: 589029 Episode Num: 711 Reward: 262.550279944995\n",
      "Total Timesteps: 590029 Episode Num: 712 Reward: 451.5687523632698\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 344.517966\n",
      "---------------------------------------\n",
      "Total Timesteps: 591029 Episode Num: 713 Reward: 401.97743576470145\n",
      "Total Timesteps: 592029 Episode Num: 714 Reward: 229.04290383295174\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 257.777047\n",
      "---------------------------------------\n",
      "Total Timesteps: 593029 Episode Num: 715 Reward: 316.2586368951409\n",
      "Total Timesteps: 594029 Episode Num: 716 Reward: 142.60399999706576\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 363.840236\n",
      "---------------------------------------\n",
      "Total Timesteps: 595029 Episode Num: 717 Reward: 443.69156886671306\n",
      "Total Timesteps: 596029 Episode Num: 718 Reward: 390.9103359606015\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 316.733057\n",
      "---------------------------------------\n",
      "Total Timesteps: 597029 Episode Num: 719 Reward: 361.3740173527652\n",
      "Total Timesteps: 598029 Episode Num: 720 Reward: 289.96688892411476\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 215.008715\n",
      "---------------------------------------\n",
      "Total Timesteps: 599029 Episode Num: 721 Reward: 286.76147071738785\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 271.550963\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# We start the main loop over 500,000 timesteps\n",
    "while total_timesteps < max_timesteps:\n",
    "  \n",
    "  # If the episode is done\n",
    "  if done:\n",
    "\n",
    "    # If we are not at the very beginning, we start the training process of the model\n",
    "    if total_timesteps != 0:\n",
    "      print(\"Total Timesteps: {} Episode Num: {} Reward: {}\".format(total_timesteps, episode_num, episode_reward))\n",
    "      policy.train(replay_buffer, episode_timesteps, batch_size, discount, tau, policy_noise, noise_clip, policy_freq)\n",
    "\n",
    "    # We evaluate the episode and we save the policy\n",
    "    if timesteps_since_eval >= eval_freq:\n",
    "      timesteps_since_eval %= eval_freq\n",
    "      evaluations.append(evaluate_policy(policy))\n",
    "      policy.save(file_name, directory=\"./pytorch_models\")\n",
    "      np.save(\"./results/%s\" % (file_name), evaluations)\n",
    "    \n",
    "    # When the training step is done, we reset the state of the environment\n",
    "    obs = env.reset()\n",
    "    \n",
    "    # Set the Done to False\n",
    "    done = False\n",
    "    \n",
    "    # Set rewards and episode timesteps to zero\n",
    "    episode_reward = 0\n",
    "    episode_timesteps = 0\n",
    "    episode_num += 1\n",
    "  \n",
    "  # Before 10000 timesteps, we play random actions\n",
    "  if total_timesteps < start_timesteps:\n",
    "    action = env.action_space.sample()\n",
    "  else: # After 10000 timesteps, we switch to the model\n",
    "    action = policy.select_action(np.array(obs))\n",
    "    # If the explore_noise parameter is not 0, we add noise to the action and we clip it\n",
    "    if expl_noise != 0:\n",
    "      action = (action + np.random.normal(0, expl_noise, size=env.action_space.shape[0])).clip(env.action_space.low, env.action_space.high)\n",
    "  \n",
    "  # The agent performs the action in the environment, then reaches the next state and receives the reward\n",
    "  new_obs, reward, done, _ = env.step(action)\n",
    "  \n",
    "  # We check if the episode is done\n",
    "  done_bool = 0 if episode_timesteps + 1 == env._max_episode_steps else float(done)\n",
    "  \n",
    "  # We increase the total reward\n",
    "  episode_reward += reward\n",
    "  \n",
    "  # We store the new transition into the Experience Replay memory (ReplayBuffer)\n",
    "  replay_buffer.add((obs, new_obs, action, reward, done_bool))\n",
    "\n",
    "  # We update the state, the episode timestep, the total timesteps, and the timesteps since the evaluation of the policy\n",
    "  obs = new_obs\n",
    "  episode_timesteps += 1\n",
    "  total_timesteps += 1\n",
    "  timesteps_since_eval += 1\n",
    "\n",
    "# We add the last policy evaluation to our list of evaluations and we save our model\n",
    "evaluations.append(evaluate_policy(policy))\n",
    "if save_models: policy.save(\"%s\" % (file_name), directory=\"./pytorch_models\")\n",
    "np.save(\"./results/%s\" % (file_name), evaluations)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TD3_Ant.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
